# 机器学习

## 感知机



感知机是二分类的线性分类模型，其输入为实例的特征向量$T$，输出为实例的类别$y$    即：


$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

$$
y=\{+1,-1\}
$$

感知机有如下几何解释：线性方程
$$
wx+b=0
$$
对应于特征空间$R^n$中的一个超平面$S$，其中$\omega$是超平面的法向量，$b$是超平面的截距。

因此这个超平面将特征空间划分为两个部分。位于两部分的点分别被分为正、负两类。
$$
sign(x)=
\begin{cases}
1& \text{x>0}\\
0& \text{x<=0}
\end{cases}
$$

$$
f(x)=sign(wx+b)
$$

$$
w \leftarrow w +\eta y_{i}x_{i} 
$$

$$
b \leftarrow b + \eta y_{i}
$$




> - $l_0$范数--向量中非0的元素个数
> - $l_1$范数--向量中各个元素绝对值之和
> - $l_2$范数--向量中各个元素平方和再开方
> - $\infty$范数--向量中各个元素绝对值最大值
> - $-\infty$范数--向量中各个元素绝对值最小值

##   K-近邻算法

### 距离度量

##### 特征空间中两个实例点的距离是两个实例点相似程度的反映！

我们定义$L_{p}$距离：
$$
L_{p}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|^{p}})^{\frac{1}{p}}
$$

- 当$p=2$时，称为欧式距离($Euclidean \space distance$)，即

$$
L_{2}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|^{2}})^{\frac{1}{2}}
$$

- 当$p=1$时，称为曼哈顿距离($Manhattan \space distance$)，即
  $$
  L_{1}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|})
  $$

- 当$p=∞$时，它代表了各个坐标距离的最大值，即
  $$
  L_{p}(x_{i},x_{j})=max_{l}{|x_{i}^{(l)}-x_{j}^{(l)}|})
  $$





## 朴素贝叶斯

##### 先验概率分布：

$$
P(Y=c_k),k=1,2,...K
$$

##### 条件概率分布：

$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)
$$



于是我们就可以学习到联合概率分布$P(X,Y)$

朴素贝叶斯对条件概率作了条件概率独立性的假设，具体地，条件独立性假设是：


$$
\begin{equation}
\begin{split}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\
&=\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k) \\
\end{split}
\end{equation}
$$


朴素贝叶斯分类时，对于给定输入$x$，通过学习后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$的输出，即：
$$
P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}{P(X=x|Y=c_k)P(Y=c_k)}}
$$
将**条件概率分布**代入：
$$
P(Y=c_k|X=x)=\frac{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_{k}{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}}
$$
因此我们可以把朴素贝叶斯分类起表示为：
$$
P(Y=c_k|X=x)=arg\space max_{c_k}\frac{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_{k}{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}}
$$


注意，上式中分母对所有$c_k$都是相同的，所以：
$$
P(Y=c_k|X=x)=arg\space max_{c_k}\varPi_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
$$









## 决策树

- #### 熵$(entropy)$：

  熵是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：
  $$
  P(X=x_i)=p_i,\space\space i=1,2,...n
  $$
  则随机变量$X$的熵的定义为（熵只依赖于$X$的分布，也将$X$的熵记作$H(p)$）：
  $$
  H(X)=H(p)=-\sum{}_{i=1}^np_{i}logp_i
  $$
  定义$0log0=0$

  熵越大，随机变量的不确定性就越大



- #### 条件熵$(conditional\space entropy)$：

  设随机变量$(X,Y)$，其联合概率分布为：
  $$
  P(X=x_i,Y=y_i)=p_{ij},\space\space i=1,2,...,n; \space\space j=1,2,...,m
  $$
  条件熵 $H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性；随机变量$X$的条件下随机变量$Y$条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的期望：
  $$
  H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)
  $$
  这里，$p_i=p(X=x_i),\space\space i=1,2,...,n$

- #### 信息增益$(information\space gain)$：

  概念：表示得知特征$X$的信息而使得类$Y$的信息不确定性的减少

  定义：特征$A$对训练数据集$D$的信息增益$g=(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：
  $$
  g(D,A)=H(D)-H(D|A)
  $$



如何度量信息量的大小？
答：信息增益



## 逻辑斯蒂回归