# 机器学习

## 感知机



感知机是二分类的线性分类模型，其输入为实例的特征向量$T$，输出为实例的类别$y$    即：


$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

$$
y=\{+1,-1\}
$$

感知机有如下几何解释：线性方程
$$
wx+b=0
$$
对应于特征空间$R^n$中的一个超平面$S$，其中$\omega$是超平面的法向量，$b$是超平面的截距。

因此这个超平面将特征空间划分为两个部分。位于两部分的点分别被分为正、负两类。
$$
sign(x)=
\begin{cases}
1& \text{x>0}\\
0& \text{x<=0}
\end{cases}
$$

$$
f(x)=sign(wx+b)
$$

$$
w \leftarrow w +\eta y_{i}x_{i} 
$$

$$
b \leftarrow b + \eta y_{i}
$$




> - $l_0$范数--向量中非0的元素个数
> - $l_1$范数--向量中各个元素绝对值之和
> - $l_2$范数--向量中各个元素平方和再开方
> - $\infty$范数--向量中各个元素绝对值最大值
> - $-\infty$范数--向量中各个元素绝对值最小值

##   K-近邻算法

### 距离度量

##### 特征空间中两个实例点的距离是两个实例点相似程度的反映！

我们定义$L_{p}$距离：
$$
L_{p}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|^{p}})^{\frac{1}{p}}
$$

- 当$p=2$时，称为欧式距离($Euclidean \space distance$)，即

$$
L_{2}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|^{2}})^{\frac{1}{2}}
$$

- 当$p=1$时，称为曼哈顿距离($Manhattan \space distance$)，即
  $$
  L_{1}(x_{i},x_{j})=(\sum_{l=1}^{n}{|x_{i}^{(l)}-x_{j}^{(l)}|})
  $$

- 当$p=∞$时，它代表了各个坐标距离的最大值，即
  $$
  L_{p}(x_{i},x_{j})=max_{l}{|x_{i}^{(l)}-x_{j}^{(l)}|})
  $$





## 朴素贝叶斯

> ##### 先验概率分布：
>
> $$
> P(Y=c_k),k=1,2,...K
> $$
>
> ##### 条件概率分布：
>
> $$
> P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)
> $$
>
> 
>
> 于是我们就可以学习到联合概率分布$P(X,Y)$
>
> 朴素贝叶斯对条件概率作了条件概率独立性的假设，具体地，条件独立性假设是：
>
>
> $$
> \begin{equation}
> \begin{split}
> P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\
> &=\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k) \\
> \end{split}
> \end{equation}
> $$
>
>
> 朴素贝叶斯分类时，对于给定输入$x$，通过学习后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$的输出，即：
> $$
> P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}{P(X=x|Y=c_k)P(Y=c_k)}}
> $$
> 将**条件概率分布**代入：
> $$
> P(Y=c_k|X=x)=\frac{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_{k}{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}}
> $$
> 因此我们可以把朴素贝叶斯分类起表示为：
> $$
> P(Y=c_k|X=x)=arg\space \max_{c_k}\frac{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}{\sum_{k}{\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)}}
> $$
>
>
> 注意，上式中分母对所有$c_k$都是相同的，所以：
> $$
> P(Y=c_k|X=x)=arg\space \max_{c_k}\varPi_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
> $$
>
###### 朴素贝叶斯算法

1. 计算先验概率及条件概率：

$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N},\space k=1,2,\cdots ,K
$$

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}
\\
\\
j=1,2,\cdots ,n;\space l=1,2,\cdots ,S_j;\space k=1,2,\cdots ,K
\\
\mbox{j表示X的第j个特征，l表示该特征可能取得的第l个值，k表示第k个类别}
\\
\mbox{则n表示共n个特征，$S_j$表示共个$S_j$特征取值，K表示共K个类别}
$$

2. 计算：

$$
P(Y=c_k|X=x)=\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
$$

则
$$
y=arg\space \max_{c_k}\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
$$


###### 贝叶斯估计

1. 计算先验概率及条件概率：

$$
P_\lambda(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda},\space k=1,2,\cdots ,K
$$

$$
P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^{N}I(y_i=c_k)+S_j\lambda}
\\
\\
j=1,2,\cdots ,n;\space l=1,2,\cdots ,S_j;\space k=1,2,\cdots ,K
\\
\mbox{j表示X的第j个特征，l表示该特征可能取得的第l个值，k表示第k个类别}
\\
\mbox{则n表示共n个特征，$S_j$表示共个$S_j$特征取值，K表示共K个类别}
\\
\mbox{$\lambda=1$时称拉普拉斯平滑}
$$

2. 计算：

$$
P(Y=c_k|X=x)=\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
$$

则
$$
y=arg\space \max_{c_k}\varPi_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)P(Y=c_k)
$$










## 决策树

- #### 熵$(entropy)$：

  熵是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：
  $$
  P(X=x_i)=p_i,\space\space i=1,2,...n
  $$
  则随机变量$X$的熵的定义为（熵只依赖于$X$的分布，也将$X$的熵记作$H(p)$）：
  $$
  H(X)=H(p)=-\sum{}_{i=1}^np_{i}logp_i
  $$
  定义$0log0=0$

  熵越大，随机变量的不确定性就越大



- #### 条件熵$(conditional\space entropy)$：

  设随机变量$(X,Y)$，其联合概率分布为：
  $$
  P(X=x_i,Y=y_i)=p_{ij},\space\space i=1,2,...,n; \space\space j=1,2,...,m
  $$
  条件熵 $H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性；随机变量$X$的条件下随机变量$Y$条件熵，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的期望：
  $$
  H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)
  $$
  这里，$p_i=p(X=x_i),\space\space i=1,2,...,n$

- #### 信息增益$(information\space gain)$：

  概念：表示得知特征$X$的信息而使得类$Y$的信息不确定性的减少

  定义：特征$A$对训练数据集$D$的信息增益$g=(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：
  $$
  g(D,A)=H(D)-H(D|A)
  $$



- 设训练数据集为$D,|D|$表示其样本容量，即**样本个数**；
- 设有K个类$C_k, k = 1,2,\cdots ,K,|C_k |$为属于**类$C_k$的样本个数**；
- 特征A有n个不同的取值$\{a_1,a_2,\cdots ,a_n\}$根据特征A的取值将D划分为n个子集$D_1,D_2,\cdots D_n,|D_i|$为 **$D_i$的样本个数**
- 记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$,$|D_{ik}|$为$D_{ik}$的样本个数
- i是特征，k是类别

$\sum_{k=1}^{K}|C_k|=|D|$

$\sum_{i=1}^{n}|D_i|=|D|$

经验熵
$$
H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log_2 \frac{|C_k|}{|D|}
$$
条件经验熵
$$
H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log_2 \frac{|D_{ik}|}{|D_i|}
$$
如何度量信息量的大小？
答：信息增益

###### 信息增益比

$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
\\
H_A(D)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
\\
\mbox{n是特征A取值的个数}
$$



## 支持向量机

#### 线性可分支持向量机学习算法--最大间隔法

(1). 构造并求解约束最优化问题：

$$
\min_{w,b} \frac{1}{2}||w||^2
\\
s.t.\ \ \ \  y_i(w \cdot x_i +b)-1 \geq 0 \ \ \ i=1,2, \cdots ,N
$$

求得最优解 $w^*,b^*$。

(2). 得到分离超平面：

$$
w^* \cdot x +b^*=0
$$

分类决策函数：
$$
f(x)=sign(w^* \cdot x +b^*)
$$





#### 线性可分支持向量机学习算法--对偶问题

(1). 构造并求解约束最优化问题：

$$
\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_i \cdot x_j)-\sum_{i=1}^{N}\alpha_i
\\
s.t.\ \ \ \  \sum_{i=1}^{N} \alpha_iy_i =0
\\
\alpha_i \geq 0 \ \ \ i=1,2, \cdots ,N
$$

求得最优解 $\alpha^*=\{\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*\}^T$。

(2). 计算
$$
w^*=\sum_{i=1}^{N}\alpha_i^*y_ix_i
$$
并选择$\alpha^*$的一个正分量$\alpha_j^*>0$，计算
$$
b^*=y_i - \sum_{i=1}^{N} \alpha_i^*y_i(x_i \cdot x_j)
$$
(3). 得到分离超平面：
$$
w^* \cdot x +b^*=0
$$

分类决策函数：
$$
f(x)=sign(w^* \cdot x +b^*)
$$



#### 线性支持向量机

###### 原始形式

$$
\min_{w,b,\xi} \frac{1}{2}||w||^2 +C\sum_{i=1}^{N}\xi_i
\\
s.t.\ \ \ \  y_i(w \cdot x_i +b) \geq 1-\xi_i \ \ \ 
\\
\xi_i \geq 0,i=1,2, \cdots ,N
$$

###### 对偶形式

$$
\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_i \cdot x_j)-\sum_{i=1}^{N}\alpha_i
\\
s.t.\ \ \ \  \sum_{i=1}^{N} \alpha_iy_i =0
\\
0 \leq \alpha_i \leq C \ \ \ i=1,2, \cdots ,N
$$

求得最优解 $\alpha^*=\{\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*\}^T$。

(2). 计算
$$
w^*=\sum_{i=1}^{N}\alpha_i^*y_ix_i
$$
并选择$\alpha^*$的一个正分量$0<\alpha_j^*<C$，计算
$$
b^*=y_i - \sum_{i=1}^{N} \alpha_i^*y_i(x_i \cdot x_j)
$$
(3). 得到分离超平面：
$$
w^* \cdot x +b^*=0
$$

分类决策函数：
$$
f(x)=sign(w^* \cdot x +b^*)
$$



## 逻辑斯蒂回归

略 放弃

现在，让我们将线性回归模型转化为逻辑回归模型--二项逻辑斯蒂回归模型
$$
\begin{equation}
\begin{aligned}
y & = w \cdot x+b\\
\\
ln(\frac{P}{1-P}) & =w \cdot x+b\\
\frac{P}{1-P} & = e^{w \cdot x+b}\\
P(Y=1|x) & = \frac{e^{w \cdot x+b}}{1+e^{w \cdot x+b}}\\

P(Y=0|x) & = \frac{1}{1+e^{-y}}
\end{aligned}
\end{equation}
$$




## 提升方法

略 放弃



## EM 算法

对数似然函数
$$
L(\theta)=\log P(Y|\theta)=\log \sum_ZP(Y,Z|\theta)=\log (\sum_ZP(Y|Z,\theta)P(Z|\theta))
$$
Y 是观测量

Q函数
$$
Q(\theta,\theta^{(i)})=E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]
\\
=\sum_Z\log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
$$


## 隐马尔科夫模型

##### 一. 定义

马尔科夫模型由初始概率分布、状态转移概率分布、观测概率分布确定。
>
> Q：所有可能状态的集合
>
> $Q=\{ q_1,q_2,\cdots ,q_N\}$，N是可能的状态数。
>
> V：所有可能观测的集合
>
> $V=\{ v_1,v_2,\cdots ,v_M\}$，M是可能的观测数。
>
> I : 长度为T的状态序列，O：是对应的观测序列
>
> $I=\{ i_1,i_2,\cdots ,i_T\}$，$O=\{ o_1,o_2,\cdots ,o_T\}$

A是状态转移概率矩阵：
$$
A=[a_{ij}]_{N\times N}
\\
a_{ij}=P(i_{t+1}=q_j|i_t=q_i),\space i=1,2,\cdots ,N;\space j=1,2,\cdots ,N
$$
注：t表示时刻或第几次；$a_{ij}记为a_{i \rightarrow j}比较好$表示从状态$q_i到 q_j$的转移概率。

B是观测转移概率矩阵：
$$
B=[b_{j}(k)]_{N\times M}
\\
b_{j}(k)=P(o_{t}=v_k|i_t=q_j),\space k=1,2,\cdots ,M;\space j=1,2,\cdots ,N
$$
注：t表示时刻或第几次；k表示观测的分量，j表示$q_j$状态，则(jk)就表示在$q_j$状态下，观测到$v_k$观测量的概率。

$\pi$是初始状态概率向量：
$$
\pi=(\pi_i)
\\
\pi_i=P(i_1=q_i),\space i=1,2,\cdots,N
$$
t=1时状态$q_i$的概率。

则隐马尔科夫模型$\lambda$可表示为：
$$
\lambda=(A,B,\pi)
$$
$A,B,\pi$即隐马尔科夫模型三要素。

###### 前向算法

前向概率

已知观测序列$O=\{o_1,o_2,\cdots ,o_T \}$
$$
\alpha_t(i)=P(o_1,o_2,\cdots ,o_t,i_t=q_i|\lambda)
$$
即在$q_i$状态下观测到$o_1,o_2,\cdots ,o_t$的概率。i表示状态，t表示序列。

算法流程
$$
\alpha_1(i)=\pi_ib_i(o_1),\space i=1,2,\cdots,N \\在不同状态下观察到o_1的概率
\\ 
\alpha_{t+1}(i)=[\sum_{j=1}^{N}\alpha_t(j)a_{ji}]b_{i}(o_{t+1}),\space i=1,2,\cdots,N
\\在已知序列o_1,o_2,\cdots ,o_t的情况下，不同状态下q_i观测到o_{t+1}的概率。即求全部状态到q_i状态的概率和。
\\
P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)\space i=1,2,\cdots,N
$$
即在全部状态Q下，对给出的观测集合O进行概率计算；亦即从不同状态下观测到$o_t$的概率和。直到递推到最后一个$o_T$。

###### 后向算法

后向概率

已知观测序列$O=\{o_1,o_2,\cdots ,o_T \}$
$$
\beta_t(i)=P(o_{t+1},o_{t+2},\cdots ,o_T|i_t=q_i,\lambda)
$$
即在时刻t，状态$q_i$的条件下观测到$o_{t+1},o_{t+2},\cdots ,o_T$的概率。i表示状态，t表示序列。注意不包括$o_t$。

算法流程
$$
\beta_T(i)  =1,\space i=1,2,\cdots,N \\在不同状态下观察到o_T的概率均为1
\\ 
\beta_{t}(i)  =\sum_{j=1}^{N}a_{ij}\beta_{t+1}(j)b_{j}(o_{t+1}),\space i=1,2,\cdots,N
\\在已知序列o_{t+2},o_{t+3},\cdots ,o_T的情况下，在状态下q_i下观测值o_{t+1}转移到o_{t+2},o_{t+3},\cdots ,o_T的概率。
\\即求q_i状态下观测值o_{t+1}到全部状态下序列o_{t+2},o_{t+3},\cdots ,o_T的概率和。这里的重点是o_{t+1}是怎么来的，
\\
P(O|\lambda)  =\sum_{i=1}^{N}\pi_{i}\beta_1(i)b_{i}(o_1)  \space i=1,2,\cdots,N
$$

在当前状态$q_i$下，得到$o_{t+1}$的概率和。得到$o_{t+1}$即从状态$q_i$到全部状态下观测到$o_{t+1}$的概率和，$i \rightarrow \sum_j^N,a_{i \rightarrow j}*b_j(o_{t+1})\beta_{t+1}(j) $。

###### 前向后向算法

$$
\begin{equation}
\begin{aligned}
时刻t处于状态q_i的概率：\space
\gamma_t(i)=P(i_t=q_i|O,\lambda)
\\
\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}
\\相当于左右相连接：\
\alpha_t(i) \beta_t(i)=P(i_t=q_i,O|\lambda)
\\
\gamma_t(i)=\frac{\alpha_t(i) \beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j) \beta_t(j)}
\end{aligned}
\end{equation}
$$



###### 维特比算法

输入：模型$\lambda=(A,B,\pi)$，观测序列$O=\{o_1,o_2,\cdots ,o_T \}$；

输出：最优路径 $I^*=\{i_1^*,i_2^*,\cdots,i_T^*\}$。

(1). 初始化
$$
\begin{equation}
\begin{aligned}
\delta_1(i) & =\pi_ib_i(o_1),\ i=1,2,\cdots,N
\\
\Psi_1(i) & =0,\ i=1,2,\cdots,N
\end{aligned}
\end{equation}
$$
(2). 递推
$$
\begin{equation}
\begin{aligned}
\delta_t(i) & =\max_{1\leq j \leq N}[a_{ji}\delta_{t-1}(j)] b_i(o_1),\ i=1,2,\cdots,N
\\
\Psi_t(i) & =\arg \max_{1\leq j \leq N}[a_{ji}\delta_{t-1}(j)],\ i=1,2,\cdots,N
\end{aligned}
\end{equation}
$$
(3). 终止
$$
\begin{equation}
\begin{aligned}
P^* & =\max_{1\leq i \leq N}\delta_T(i)
\\
i^* &=\arg\max_{1\leq i \leq N}[\delta_T(i)]
\end{aligned}
\end{equation}
$$
(4). 最优路径回溯。对$t=T-1,T-2,\cdots,1$
$$
i^*=\Psi_{t+1}(i_{t+1}^*)
$$
得最优路径 $I^*=\{i_1^*,i_2^*,\cdots,i_T^*\}$。