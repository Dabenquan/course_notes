# 大规模数据处理系统

------

*2021-1-11*

==绪论、HDFS、MapReduce、Spark、Yarn、Flink==

------

## 第一章 绪论

##### 分布式系统是什么？

若干独立计算机的集合。这些计算机对于用户来说就像是一个单机的系统。

##### 分布式系统的类型

###### 基于计算机构建的分布式系统

1. 分布式计算系统

   > 分布式计算系统是用于实现高性能计算的系统，所说的高性能计算主要面向解决科学计算领域的复杂数值计算问题，支持大规模数据处理，分布式计算系统是作用于若干独立计算机之上、使得这些计算机能够协同执行计算完成某项应用的软件系统。

- 集群计算：同构，同一个业务，部署在多个服务器上

- 网格计算：以“集中力量办大事”为核心思想的分布式计算架构，它通过整合分布的同构或异构的超算资源以解决重大科研问题。主要应用场景为科学计算，多见于各大超算中心。

- 云计算

  *注：分布式：一个业务分拆多个子业务，部署在不同的服务器上 异构*

2. 分布式信息系统

- 事务处理

- 企业信息集成

  > - **高吞吐量（throughput）**
  > - **高可用（availability）**

###### 数据管理系统发展历史

1. 第一代：层次、网状数据库系统
   	*数据的存储与访问从应用程序中分离*
2. 第二代：关系数据库系统
   	*OLTP:联机事务处理*
3. 第三代：数据仓库系统
   	*OLAP:联机分析处理*
4. 第四代：大数据管理系统
   	*分布式、可扩展、高可用*

###### 大数据特点（5V）

- ​	Volume(数量大)
- ​	Variety(种类多)
- ​	Velocity(速度快)
- ​	Value(价值高)
- ​	Veracity(质量低)

> 从“一劳永逸”走向“分类定制”：解耦数据库系统的各个模块，并从模型、可靠性、可伸缩性、
> 性能等方面对各个模块进行重新设计

|       集中式       |                  |                  分布式                   |
| :----------------: | ---------------- | :---------------------------------------: |
|      数据存储      | 分布式存储系统   |          HDFS、Kafka、Alluxio等           |
| 数据组织模型与访问 | 分布式NoSQL系统  | BigTable、Dynamo、<br/>Cassandra、Neo4j等 |
|      查询处理      | 分布式计算系统   |         MapReduce、Spark、Flink等         |
|      事务管理      | 分布式NewSQL系统 |        Spanner、OceanBase、TiDB等         |

###### 面向数据密集型应用的分布式计算系统

|     应用     |     类型     |             代表性系统              |
| :----------: | :----------: | :---------------------------------: |
|              |  批处理系统  |    **==MapReduce、Spark==**、Tez    |
| 通用数据处理 |  流计算系统  |       Storm、Spark Streaming        |
|              | 批流融合系统 | **==Flink==**、Structured Streaming |
|              |  图处理系统  |           Giraph、GraphX            |
| 领域数据处理 | 机器学习系统 |       Mahout、SystemDS、Mllib       |
|              | 查询分析系统 |              Hive、Pig              |



> 计算密集型应用：计算机在处理这些应用时CPU的处理能力成为了首要限制性因素
> - 高性能计算领域的数值运算
>
> - 人工智能领域的深度学习模型训练
>   
>
> 数据密集型应用：计算机在处理这些应用时I/O带宽（包括磁盘I/O、网络I/O等）成为了首要限制性因素
>
> -  	大数据领域的数据处理
>

> where-应用场景、why-目的、object-数据模型、method-计算模型

------

## 第二章 Hadoop

- **HDFS: Hadoop Distributed File System(分布式文件系统)** 

- **MapReduce:分布式计算框架**

  ------

### HDFS

##### HDFS的结构

![image-20210111164542989](.\images\HDFS架构图.png)

###### HDFS client：

- 文件切分：文件上传 HDFS的时候，Client 将文件切分成一个个Block（数据块），然后进行存储。
- 与NameNode 交互：获取文件真实的位置信息。
- 与DataNode 交互：读取或写入数据。
- Client 提供一些命令来管理 和访问HDFS，比如启动或者关闭HDFS。

###### NameNode:

*负责HDFS的管理工作，包括管理文件目录结构、位置等元数据、维护 DataNode的状态等并不实际存储文件。*

*NameNode中的元数据信息存储在内存/磁盘中，内存中为实时信息，磁盘中为数据的持久化存储使用使用；存储元数据信息，（元数据存储两份，一份在内存中，一份在硬盘中，保存文件、block、DataNode的映射关系）*

>  磁盘中存储的信息主要下面两个
>
> 1. fsimage：元数据的镜像文件，存储NameNode元数据信息
> 2. edit：操作日志文件（比如你上次，追加内容，这里只有写操作的日志，读操作不会记录）
>
> **edit**：
>
> - 存放了客户端最近一段时间的操作日志
> - 客户端对 HDFS 进行写文件时的操作会先被记录在 edits 文件中
> - edits 修改时元数据也会更新
>
> **fsimage**：
>
> - NameNode 中关于元数据的镜像，一般称为检查点，fsimage 存放了一份比较完整的元数据信息
> - 因为 fsimage 是 NameNode 的完整的镜像, 如果每次都加载到内存生成树状拓扑结构，这是非常耗内存和CPU, 所以一般开始时对NameNode 的操作都放在 edits 中
> - fsimage 内包含了 NameNode 管理下的所有 DataNode 文件和文件 block 以及 block所在的 DataNode 的元数据信息
> - 随着 edits 内容增大，就需要在一定策略下和 fsimage 合并
>
> *由于集群重启时NameNode会重新加载fsimage和edits文件，fsimage文件加载起来很快，但edits文件内记录的都是操作日志，因此edits文件不能无限增长，否则重放日志很慢，会影响到集群启动的速度，因此edits文件和fsimage会定期进行合并。**一般NameNode会持续运行，不会被启动，那么edit文件会增长很大，这个时候就不好管理。如果edit文件增长到很大，那么每次NameNode启动合并edit文件和fsimage就会很久，那NameNode启动就会很慢*![图片](.\images\NN.jpg)
>
> 基于这样的原因，SNN（second NameNode）作为NN的协助者，帮助进行元数据合并。见下文。

- 管理 HDFS 的名称空间
- 管理数据块（Block）映射信息
- 配置副本策略•处理客户端读写请求
- 周期性的接收心跳和块的状态信息报告

###### Seconedary NameNode:

*Secondary NameNode不是NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务，而是作为一个辅助者分担NameNode的工作量。*

- 定期合并 fsimage和edits，并推送给NameNode，把 edits 控制在一个范围内
- 在紧急情况下，可辅助恢复 NameNode

###### DataNode:

*负责数据块的存储，为客户端提供实际文件数据*

- 存储实际的数据块
- 执行数据块的读/写操作
- 周期性向NameNode汇报心跳信息
- 周期性向NameNode汇报数据块信息
- 周期性向NameNode汇报缓存数据块信息

> 数据存储在内存中是为了读取性能，保证效率，数据存储在硬盘中，为了持久化数据，保证数据不丢失。
>
> DataNode通过向NameNode发送心跳保持与其联系（3秒一次），如果NameNode 10分钟没有收到DataNode的心跳，则认为其已经lost，并复制其上的block到其它DataNode。

###### Block

*数据块（block）是HDFS存储文件的基本单位*

*在HDFS中，有一个特别重要的概念，数据块（block），前面介绍过，在HDFS存储的文件都是超大数据的文件，我们可以把这个超大规模的文件以一个标准切分成几块，分别存储到不同的磁盘上，这个标准就是block。*

- 为了存储大文件，一个服务器很难存储超大型的文件，拆分的话，文件块可以保存在不同的磁盘，在HDFS文件系统中，一个文件可以分成不同的block存储在不同的磁盘上
- 简化存储系统，这样就不需要管理文件，而是直接管理文件块就可以了
- 有利于数据的复制，在HDFS系统中，一个block块一般会复制三份（可以修改），比如复制一个1TB的数据和复制多个128MB的文件复制哪个更快？

> *对于一个文件而言，一个block id从0开始，按照固定的大小，顺序对文件进行划分和编号，划分好的每一块称一个block。HDFS默认的block的大小是128MB（新版本，旧版本64MB），所以一个256MB的文件，共有256/128=2个块。不同于普通的文件系统（比如ext4或者NTFS），HDFS中，如果一个文件小于一个数据块的大小，并不用占用整个数据存储空间，而是仅仅会占用文件实际大小的空间*。*大部分HDFS操作文件时，需要一次 写入，多次读取，在HDFS文件系统中，一个文件块一旦经过 创建，写入，关闭后就不允许 修改了，在hdfs2.7后 ，才允许对block进行追加修改，即append操作，但是不能改变已有的数据，这样简单的一致性模型，保证数据操作的简单化。*

###### 文件块存放策略（机架感知）

- 第一个block副本放在客户端所在的数据节点里（如果客户端不在集群范围内，则从整个集群中随机选择一个合适的数据节点来存放）
- 第二个副本放置在与第一个副本所在节点不同机架内的某一数据节点上（随机选择）
- 第三个副本放置在第一个副本所在机架的不同节点上
- 如果还有其他副本，则随机放在其他节点上

> 这样设计的好处是：
> 当本地数据损坏时，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；当整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。

###### NameNode与Secondary NameNode

![image-20210111182651416](.\images\DN&SNN.png)

当edit和fsimage文件合并策略触发时，合并流程如下：

1. SNN（Secondary NameNode）通知NN（NameNode）暂时切换将日志写到edits.new内
2. SNN通过GET请求将NN中的edits和fsimage文件加载到内存中
3. SNN将内存中的edits和fsimage合并生成新的fsimage.ckpt文件
4. SNN通过POST请求将生成的fsimage.ckpt文件传给NN
5. NN将收到的fsimage.ckpt替换旧的fsimage文件
6. NN将edits.new替换旧的edits文件

> 触发checkpoint。主要由core-site.xml文件中的两个参数来管理：
> a. 默认是3600s合并一次，可以通过修改fs.checkpoint.period自定义
> b. 根据edit.log文件的大小触发合并，默认是64MB会触发合并，可以通过修改fs.checkpoint.size自定义
>
>
> ```xml
> <!-- 多久记录一次 HDFS 镜像, 默认 1小时 -->
> <property>
> <name>fs.checkpoint.period</name>
> <value>3600</value></property>
> <!-- 一次记录多大, 默认 64M -->
> <property>
> <name>fs.checkpoint.size</name>
> <value>67108864</value>
> </property>
> ```

##### HDFS的执行流程

###### 读数据流程

![图片](.\images\读.jpg)

1. client向远程ＮameＮode发起读请求
2. NN会视情况返回文件的部分或者全部block列表，对于每个block，NameNode都会返回该block的地址和副本的DN的地址
3. 客户端会选取最接近的DN来读取block
4. 读取完当前的block的数据后，关闭与当前的DN的连接，并为读取下一个block寻找最佳的DN
5. 当读完列表的block后，且文件读取还没有结束，客户端会继续向NN获取下一批的block列表
6. 读取完一个block都会进行checksum验证，如果读取的时候出现错误，client会通知NN，然后在从下一个拥有该block块的DN继续读取数据

###### 写数据流程

![图片](.\images\写.jpg)

1. 客户端向NameNode提交写请求；
2. NN进行权限检查、判断是否满足写条件；
3. NN返回信息：可以上传；并将写操作记录在edits日志内。
4. 客户端根据HDFS的块策略将文件切分为n个block文件块；
5. 请求上传第一个文件块blk_a_1；
6. 根据DN上的block信息和机架感知，选出可以上传的DN列表：(dn1,dn2,dn4)；
7. NN返回可上传的DN列表(dn1,dn2,dn4)；
8. 客户端和DN建立数据传输管道，上传的DN之间也建立管道；
9. 客户端向DN以数据包packet(64k)传递数据，dn1收到一个packet会传给dn2，dn2收到会传给dn4，每传一个packet会放入一个应答队列等待应答；
10. DN将收到的packet数据包进行缓存；
11. 在管道的反方向上, DN逐个发送 ack（命令正确应答）, 最终由管道中第一个DN节点dn1将ack发送给客户端；
12. 当文件块block的packet传输完成则将缓存的临时文件放置在指定的HDFS上传路径；
13. 继续上传其余的block文件块；
14. 所有block上传完毕后返回完成信号给NN；

> - client向NameNode发起写请求
> - NN会检查路径是否存在、权限是否正确、文件是否存在
> - 条件满足后，client开始写入文件，首先开发库会将文件拆分成多个packets，并在内部以数据队列的形式来管理这些packet，并向NN申请新的blocks，获取用来存储block和副本的DN的列表，
> - 开始已经pipeline（管道）的形式将packet写入到第一个DN中，当第一个DN写入成功后，在将其传递给下一个DN，直到最后一个DN存储完成
> - 然后开始上传下一个packet

###### 删除流程

> 当从HDFS中删除某个文件时，这个文件并不会立刻从HDFS中删除，而是将这个文件重命名并转移到回收站 /trash目录，只要这个文件还在/trash目录下，该文件就可以迅速被恢复。回收站的位置在HDFS上的/user/$USER/.Trash/Current/
> 文件在/trash目录中存放的时间默认为6个小时，当超过这个时间时，NN就会将文件从名称空间中删除；
> 删除文件会使得该文件相关的数据块被释放；注意：从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟；

1. 现在NN上执行节点名字的删除
2. 当NN上执行delete方式时，他这是标记操作涉及需要被删除的数据块，而不是主动联系这些数据块所在的DN节点
3. 当保存这些数据库的DN节点向NN节点发送心跳时，在心跳应答里，NN会向DN发出指令，从而把数据删除
4. 所以在执行delete方法后一段时间内，数据块才会被删除掉

##### 补充高可用性

###### JournalNode

以上是HDFS的基础架构，NameNode对于HDFS很重要，整个HDFS文件系统的元数据信息都由NameNode来管理，NameNode的可用性直接决定了Hadoop 的可用性，一旦NameNode进程不能工作了，就会影响整个集群的正常使用。因此在Hadoop2.x版本中加入了HDFS HA的特性，在典型的HA集群中，两台独立的机器被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。Active NameNode负责群集中的所有客户端 操作，而Standby充当从服务器，Standby机器保持足够的状态以提供快速故障切换。

两个NameNode为了数据同步，会通过一组称作**JournalNodes**的独立进程进行相互通信。当Active 状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。Standby 状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。Standby 可以确保在集群出错时，命名空间状态已经完全同步了 ，以此达到快速故障切换。

在HA架构下，Secondary NameNode被JournalNode替代，实现两个NameNode之间的信息同步，由**Zookeeper**实现两个NameNode之间的高可用，相关的组件如下：

- **ZKFailoverController**

是基于Zookeeper的故障转移控制器，它负责控制NameNode的主备切换，ZKFailoverController会监测NameNode的健康状态，当发现Active NameNode出现异常时会通过Zookeeper进行一次新的选举，完成Active和Standby状态的切换

- **HealthMonitor**

周期性调用NameNode的HAServiceProtocol RPC接口（monitorHealth 和 getServiceStatus），监控NameNode的健康状态并向ZKFailoverController反馈；

- **ActiveStandbyElector**

接收ZKFC的选举请求，通过Zookeeper自动完成主备选举，选举完成后回调ZKFailoverController的主备切换方法对NameNode进行Active和Standby状态的切换；

- **DataNode**

NameNode包含了HDFS的元数据信息和数据块信息（blockmap），为了确保快速切换，Standby 状态的NameNode有必要知道集群中所有数据块的位置。为了做到这点，所有的DataNode必须配置两个NameNode的地址，同时发送数据块位置信息和心跳给他们两个。

- **共享存储系统（JournalNode）**

共享存储系统负责存储HDFS的元数据（EditsLog），Active NameNode（写入）和 Standby NameNode（读取）通过共享存储系统实现元数据同步，在主备切换过程中，新的Active NameNode必须确保元数据同步完成才能对外提供服务；对于HA集群而言，确保同一时刻只有一个NameNode处于active状态是至关重要的。否则，两个NameNode的数据状态就会产生分歧，可能丢失数据，或者产生错误的结果。为了保证这点，JNs必须确保同一时刻只有一个NameNode可以向自己写数据。

![图片](.\images\HDFS高可用架构.jpg)

> Hadoop3.x版本中的新特性允许2个以上的NameNode节点，该功能能够通过运行更多Standby NameNode来提供更高的容错性，满足一些部署的需求。比如，通过配置3个NameNode和5个JournalNode，集群能够满足允许两个节点故障的容错。

### MapReduce

- **Map：映射过程，把一组数据按照某种Map函数映射成新的数据。**

- **Reduce：归约过程，把若干组映射结果进行汇总并输出。**


##### 数据模型

![image-20210112152404735](.\images\mapReduce数据模型.png)

##### Shuffle 阶段

Hadoop MapReduce 的 Shuffle 阶段是指从 Map 的输出开始，包括系统执行排序，以及传送 Map 输出到 Reduce 作为输入的过程。

排序阶段是指对 Map 端输出的 Key 进行排序的过程。不同的 Map 可能输出相同的 Key，相同的 Key 必须发送到同一个 Reduce 端处理。Shuffle 阶段可以分为 Map 端的 Shuffle 阶段和 Reduce 端的 Shuffle 阶段。

![Hadoop MapReduce 的 Shuffle 阶段](.\images\mapreduce-shuffle.jpg)

###### map端



​             **split -①-> map -②-> [环形缓冲区 -③-> 磁盘 ] -④-> 分区***

> ①生成map之前，会计算文件分片的大小  根据分片的大小计算map的个数，每一个分片都会产生一个map作业【或者是一个文件（小于分片大小*1.1）生成一个map作业】，然后通过自定的map方法进行自定义的逻辑计算，计算完毕后会写到本地磁盘。

> ②为了保证IO效率，不直接写入磁盘。先写入*内存的环形缓冲区*，并做一次预*排序*(快速排序)。缓冲区的大小默认为100MB（可通过修改配置项mpareduce.task.io.sort.mb进行修改），当写入内存缓冲区的大小到达一定比例时，默认为80%（可通过mapreduce.map.sort.spill.percent配置项修改）。

> ③启动一个*溢写线程*将内存缓冲区的内容溢写到*磁盘（spill to disk）*，这个溢写线程是*独立*的，不影响map向缓冲区写结果的线程，在溢写到磁盘的过程中，map继续输入到缓冲中，如果期间缓冲区被填满，则map写会被阻塞到溢写磁盘过程完成。溢写是通过*轮询*的方式将缓冲区中的内存写入到本地mapreduce.cluster.local.dir目录下。

> ④在溢写到磁盘之前，我们会知道reduce的数量，然后会根据reduce的数量*划分分区*，默认根据hashpartition对溢写的数据写入到相对应的分区。在每个分区中，后台线程会根据key进行*排序*，所以溢写到磁盘的文件是*分区且排序*的。如果有combiner函数，它在排序后的输出运行，使得map输出更紧凑。减少写到磁盘的数据和传输给reduce的数据。

> *每次环形换冲区的内存达到阈值时，就会溢写到一个新的文件，因此当一个map溢写完之后，本地会存在多个分区切排序的文件。在map完成之前会把这些文件合并成一个分区且*排序(归并排序)*的文件，可以通过参数mapreduce.task.io.sort.factor控制每次可以合并多少个文件。
>
> 在map溢写磁盘的过程中，对数据进行**压缩**可以提交速度的传输，减少磁盘io，减少存储。默认情况下不压缩，使用参数mapreduce.map.output.compress控制，压缩算法使用mapreduce.map.output.compress.codec参数控制。

###### reduce端

1. Reduce 进程启动一些数据复制线程，请求Map任务所在的 TaskTracker 以获取输出文件（Copy阶段）。
2. 将 Map 端复制过来的数据先放入内存缓冲区中，Merge 有 3 种形式，分别是内存到内存，内存到磁盘，磁盘到磁盘。默认情况下，第一种形式不启用，第二种形式一直在运行（Spill 阶段），直到结束，第三种形式生成最终的文件（Merge阶段）。
3. 最终文件可能存在于磁盘中，也可能存在于内存中，但是默认情况下是位于磁盘中的。当 Reduce 的输入文件已定，整个 Shuffle 阶段就结束了，然后就是 Reduce 执行，把结果放到 HDFS 中（Reduce阶段）。

> map任务完成后，监控作业状态的application master便知道map的执行情况，并启动reduce任务，application master并且知道map输出和主机之间的对应映射关系，reduce轮询application master便知道主机所要复制的数据。
>

> 一个Map任务的输出，可能被多个Reduce任务抓取。每个Reduce任务可能需要多个Map任务的输出作为其特殊的输入文件，而每个Map任务的完成时间可能不同，当有一个Map任务完成时，Reduce任务就开始运行。Reduce任务根据分区号在多个Map输出中抓取（fetch）对应分区的数据，这个过程也就是Shuffle的copy过程。
>

> reduce有少量的复制线程，因此能够并行的复制map的输出，默认为5个线程。可以通过参数mapreduce.reduce.shuffle.parallelcopies控制。这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作。
>

> 如果map输出很小，会被复制到Reducer所在节点的内存缓冲区，缓冲区的大小可以通过mapred-site.xml文件中的mapreduce.reduce.shuffle.input.buffer.percent指定。一旦Reducer所在节点的内存缓冲区达到阀值，或者缓冲区中的文件数达到阀值，则合并溢写到磁盘。
>

> 如果map输出较大，则直接被复制到Reducer所在节点的磁盘中。随着Reducer所在节点的磁盘中溢写文件增多，后台线程会将它们合并为更大且有序的文件。当完成复制map输出，进入sort阶段。这个阶段通过归并排序逐步将多个map输出小文件合并成大文件。最后几个通过归并合并成的大文件作为reduce的输出
>



###### 补充

在MapReduce计算框架中，主要用到两种排序算法：快速排序和归并排序。在Map任务发生了2次排序，Reduce任务发生一次排序：

1. 第1次排序发生在Map输出的内存环形缓冲区，使用快速排序。当缓冲区达到阀值时，在溢写到磁盘之前，后台线程会将缓冲区的数据划分成相应分区，在每个分区中按照键值进行内排序。

2. 第2次排序是在Map任务输出的磁盘空间上将多个溢写文件归并成一个已分区且有序的输出文件。由于溢写文件已经经过一次排序，所以合并溢写文件时只需一次归并排序即可使输出文件整体有序。

3. 第3次排序发生在Shuffle阶段，将多个复制过来的Map输出文件进行归并，同样经过一次归并排序即可得到有序文件。

##### 架构图

![image-20210112152622864](.\images\mapReduce架构图.png)

###### Client

1. **提交作业**

- 用户编写的MapReduce程序通过Client提交到JobTracker
- 用户可通过Client提供的一些接口查看作业运行状态
- 在Hadoop MapReduce的实现中，该进程的名称为RunJar

###### JobTracker

1. **资源管理**

   通过监控TaskTracker来管理系统拥有的计算资源

2. **作业管理**
   负责将作业（Job）拆分成任务（Task），并进行任务调度以及跟踪任务的运行进度、资源使用量等信息

###### TaskTracker

1. **管理本节点的资源**
   TaskTracker使用slot等量划分本节点上的资源量（CPU、内存等）
2. **执行JobTracker的命令**
   接收JobTracker发送过来的命令并执行（如启动新Task、杀死Task等）
3. **向JobTracker汇报情况**
   通过心跳将本节点上资源使用情况和任务运行进度汇报给JobTracker

###### Task
1. **任务执行**
   JobTracker根据TaskTracker汇报的信息进行调度，命令存在空闲slot的TaskTracker启动
Task进程执行map或reduce任务
   在Hadoop MapReduce的实现中该进程的名称为Child

##### 执行流程

![image-20210112155108737](.\images\mapReduce执行流程.png)

1. Client将用户编写的MapReduce作业的配置信息、jar包等信息上传到共享文件系统，通常是HDFS。
2. Client提交作业给JobTracker，即告知作业信息的位置。
3. JobTracker读取作业的信息，生成一系列Map和Reduce任务，调度给拥有空闲slot的TaskTracker。
4. TaskTracker根据JobTacker的指令启动Child进程执行Map任务，Map任务将从共享文件系统读取输入数据。
5. JobTracker从TaskTracker处获得Map任务进度信息。
6. 一旦Map任务完成后，JobTacker将Reduce任务分发给TaskTracker。
7. TaskTracker根据JobTacker的指令启动Child进程执行Reduce任务，**Reduce任务将从==Map任务所在节点的本地磁盘==拉取Map的输出结果**。
8. JobTracker从TaskTracker处获得Reduce任务进度信息。
9. 当Reduce任务运行结束并将结果写入共享文件系统，则意味着整个作业执行完毕。

##### 进阶



## 第三章 Spark







## 第四章 Yarn





## 第五章 Flink

